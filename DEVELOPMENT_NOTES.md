# üî¨ Development Notes & Technical Details

This document contains the complete development history, technical findings, errors encountered, and solutions discovered during the project. Use this as reference for troubleshooting and future development.

---

## üìã Table of Contents

- [Complete Project Journey](#complete-project-journey)
- [Critical Issues & Solutions](#critical-issues--solutions)
- [Inference Debugging Details](#inference-debugging-details)
- [Technical Configuration Reference](#technical-configuration-reference)
- [All Function Definitions](#all-function-definitions)
- [Training Results by Stage](#training-results-by-stage)
- [Development Timeline](#development-timeline)
- [Key Learnings](#key-learnings)

---

## üéØ Complete Project Journey

### Initial Approach: All 18 Functions (FAILED)

**What We Tried**:
- Trained FunctionGemma with all 18 music functions simultaneously
- 900 examples (50 per function)
- 5 epochs training

**Result**:
- Training completed with normal metrics
- **0% functional accuracy** on tests
- Model couldn't distinguish between functions
- Complete failure

**Root Cause**:
- **Cognitive overload**: 270M parameter model cannot learn 18 different function patterns simultaneously
- Too many patterns to learn at once
- Insufficient examples per function for model size

### The Breakthrough: Gradual Scaling

**Insight**: Start small, validate, then incrementally add complexity.

**Stage 1: 2 Functions** ‚úÖ
- Functions: `play_song`, `playback_control`
- 100 examples (50 per function)
- Result: **100% accuracy**
- Training time: ~2.5 minutes
- Model: [Jageen/music-2func](https://huggingface.co/Jageen/music-2func)

**Stage 2: 4 Functions** ‚úÖ
- Functions: `play_song`, `playback_control`, `search_music`, `create_playlist`
- 100 examples (25 per function)
- Result: **98.9% training, 98.5% eval accuracy**
- Training time: ~2.5 minutes
- Model: [Jageen/music-4func](https://huggingface.co/Jageen/music-4func)

**Stage 3: 8 Functions** ‚è≥ (Next)
- Add: `add_to_playlist`, `remove_from_playlist`, `shuffle`, `repeat`
- 200 examples (25 per function)
- Expected: 95%+ accuracy

**Stage 4: 18 Functions** ‚è≥ (Final Goal)
- All music control functions
- 450+ examples (25 per function)
- Expected: 90%+ accuracy

---

## ‚ö†Ô∏è Critical Issues & Solutions

### Issue 1: Incomplete LoRA Configuration (CRITICAL)

**Discovery Date**: Early development
**Severity**: CRITICAL - Complete inference failure

**Symptoms**:
- Training completes with high token accuracy (98%+)
- Model generates only `<pad>` tokens during inference
- All test cases fail (0/8 tests pass)
- Adapter file size: ~5-6MB (should be ~15MB)

**Root Cause**:
Missing LoRA target modules in configuration. Only targeted attention layers, missed feed-forward layers.

**Wrong Configuration** (CAUSES FAILURE):
```python
from peft import LoraConfig

lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj"  # Only 4 modules - WRONG!
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
```

**Result of Wrong Config**:
- Trainable params: 1.47M (0.54%)
- Adapter size: ~5.6MB
- Inference: Only `<pad>` tokens

**Correct Configuration** (WORKS):
```python
lora_config = LoraConfig(
    r=16,
    lora_alpha=32,
    target_modules=[
        "q_proj", "k_proj", "v_proj", "o_proj",      # Attention layers
        "gate_proj", "up_proj", "down_proj"           # Feed-forward layers - CRITICAL!
    ],
    lora_dropout=0.05,
    bias="none",
    task_type="CAUSAL_LM"
)
```

**Result of Correct Config**:
- Trainable params: 3,796,992 (1.40%)
- Adapter size: ~15MB
- Inference: Perfect function calls

**How to Detect This Issue**:
```bash
# Check adapter file size
ls -lh models/*/final/adapter_model.safetensors

# Good: ~15MB
# Bad:  ~5-6MB

# Check trainable parameters in training logs
# Good: 3.8M trainable params (1.40%)
# Bad:  1.5M trainable params (0.54%)
```

**Fix**:
1. Update `target_modules` in training script
2. Delete the failed model directory
3. Retrain from scratch with correct config

---

### Issue 2: Incorrect Test Script Setup (MISDIAGNOSED)

**Discovery Date**: Mid-development
**Severity**: HIGH - Led to misdiagnosis of working model

**Initial Symptoms**:
- Training succeeded with 98.5% accuracy
- Early test scripts showed only pad tokens
- Misdiagnosed as "PyTorch 2.8.0 bug"
- Suspected environment/library issue

**Actual Root Cause**:
Incorrect test script configuration and validation logic. The model was working correctly all along!

**What Was Wrong**:
1. Test scripts not properly configured for FunctionGemma format
2. Device configuration issues in some test attempts
3. Not checking for correct output format (`call:function_name` prefix)
4. Validation logic expecting wrong token format

**Discovery Process**:
Created systematic debugging script (`scripts/debug_inference.py`) that tested three different approaches:
1. Standard PEFT loading ‚Üí ‚úÖ WORKS
2. Merged adapters ‚Üí ‚úÖ WORKS
3. Different generation parameters ‚Üí ‚úÖ WORKS

**Resolution**:
Model inference works perfectly. Issue was entirely in test infrastructure, not the model or environment.

**Correct Test Setup**:
```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "google/functiongemma-270m-it",
    torch_dtype=torch.float32,  # float32 for CPU
    device_map="cpu",
    trust_remote_code=True
)

# Load tokenizer and adapter
tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)
model = PeftModel.from_pretrained(base_model, MODEL_PATH)

# Prepare input
messages = [{"role": "user", "content": "Play Bohemian Rhapsody"}]
prompt = tokenizer.apply_chat_template(
    messages,
    tools=MUSIC_FUNCTIONS,
    add_generation_prompt=True,
    tokenize=False
)
inputs = tokenizer(prompt, return_tensors="pt")

# Generate with proper parameters
outputs = model.generate(
    **inputs,
    max_new_tokens=128,
    do_sample=False,
    pad_token_id=tokenizer.eos_token_id,
    eos_token_id=tokenizer.eos_token_id
)

# Decode response
response = tokenizer.decode(
    outputs[0][inputs['input_ids'].shape[1]:],
    skip_special_tokens=False
)

# Validate correct format
assert "play_song" in response
assert "<start_function_call>" in response
assert "call:" in response
```

**Key Lesson**: Always verify test infrastructure before diagnosing model issues! The model was perfect; tests were wrong.

---

### Issue 3: PEFT + GPU + float16 Inference (CRITICAL FOR GPU)

**Discovery Date**: During Colab testing
**Severity**: CRITICAL for GPU deployment

**Symptoms**:
- Model works perfectly on CPU with float32
- Generates only pad tokens on GPU with float16
- Training succeeds but GPU inference fails
- No error messages, just wrong output

**Root Cause**:
PEFT adapters have compatibility issues with GPU + float16 precision, causing only pad token generation during inference.

**Wrong Approach** (FAILS ON GPU):
```python
# This generates only pad tokens on GPU with float16
base_model = AutoModelForCausalLM.from_pretrained(
    "google/functiongemma-270m-it",
    torch_dtype=torch.float16,  # float16 precision
    device_map="auto"            # GPU
)

model = PeftModel.from_pretrained(base_model, "Jageen/music-4func")

# Generate
outputs = model.generate(**inputs, max_new_tokens=128)
# Result: ‚ùå Only pad tokens
```

**Correct Approach** (WORKS ON GPU):
```python
# Load base model and adapter
base_model = AutoModelForCausalLM.from_pretrained(
    "google/functiongemma-270m-it",
    torch_dtype=torch.float16,
    device_map="auto"
)

peft_model = PeftModel.from_pretrained(base_model, "Jageen/music-4func")

# CRITICAL: Merge adapters before inference
model = peft_model.merge_and_unload()

# Generate
outputs = model.generate(**inputs, max_new_tokens=128)
# Result: ‚úÖ Perfect function calls
```

**Why `merge_and_unload()` Works**:
1. Merges LoRA weights directly into base model parameters
2. Eliminates PEFT adapter layer overhead
3. Creates standard model without adapter hooks
4. Avoids float16 precision issues in adapter layers
5. Faster inference (no adapter computation)

**When to Use This**:
- ‚úÖ GPU inference (Colab, cloud deployment)
- ‚úÖ float16 precision
- ‚úÖ Production deployment
- ‚úÖ Any inference that needs maximum speed

**When Not Needed**:
- CPU inference with float32 (PEFT adapters work fine)
- Development/debugging (direct PEFT loading is fine for float32)

**Implementation in Colab**:
```python
# In notebook cell
base_model = AutoModelForCausalLM.from_pretrained(
    "google/functiongemma-270m-it",
    torch_dtype=torch.float16,
    device_map="auto",
    trust_remote_code=True
)

peft_model = PeftModel.from_pretrained(base_model, "Jageen/music-4func")
model = peft_model.merge_and_unload()  # Essential for GPU!

# Now use model for inference
```

---

### Issue 4: SFTTrainer API Changes

**Discovery Date**: During training script updates
**Severity**: MEDIUM - Breaking API changes

**Error 1**: `TypeError: __init__() got an unexpected keyword argument 'tokenizer'`

**Cause**: HuggingFace TRL library changed parameter name from `tokenizer` to `processing_class`.

**Fix**:
```python
from trl import SFTTrainer

# OLD (doesn't work with newer TRL versions)
trainer = SFTTrainer(
    model=model,
    tokenizer=tokenizer,  # Old parameter name
    ...
)

# NEW (correct)
trainer = SFTTrainer(
    model=model,
    processing_class=tokenizer,  # New parameter name
    ...
)
```

**Error 2**: `TypeError: __init__() got an unexpected keyword argument 'max_seq_length'`

**Cause**: `max_seq_length` moved out of `SFTConfig` in newer versions.

**Fix**:
```python
from trl import SFTConfig

# OLD (doesn't work)
training_args = SFTConfig(
    max_seq_length=2048,  # Don't use this here
    output_dir=str(output_dir),
    ...
)

# NEW (correct)
training_args = SFTConfig(
    output_dir=str(output_dir),
    num_train_epochs=5,
    # max_seq_length removed from SFTConfig
    ...
)
```

---

### Issue 5: Missing formatting_func (SUBTLE BUT CRITICAL)

**Discovery Date**: During initial training attempts
**Severity**: HIGH - Silent failure with misleading metrics

**Symptoms**:
- High token accuracy during training (98%+)
- Model generates function **definitions** instead of calls
- 0% functional accuracy despite high token accuracy
- Output: `type:<escape>STRING<escape>},song_name:{description:...`

**Root Cause**:
`SFTTrainer` not using pre-formatted text from dataset. Without `formatting_func`, trainer processes raw data incorrectly.

**Wrong Setup** (SILENT FAILURE):
```python
from trl import SFTTrainer

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    processing_class=tokenizer,
    # Missing formatting_func - trains on raw data!
    ...
)
```

**Result**: Model learns schema format instead of function calling pattern.

**Correct Setup**:
```python
def formatting_func(example):
    """Extract pre-formatted text from dataset."""
    return example["text"]

trainer = SFTTrainer(
    model=model,
    train_dataset=train_dataset,
    processing_class=tokenizer,
    formatting_func=formatting_func,  # REQUIRED!
    ...
)
```

**Why This Matters**:
- Dataset contains pre-formatted FunctionGemma text in `"text"` field
- Without `formatting_func`, trainer doesn't know to use this field
- Trainer processes raw message dictionaries instead
- Results in completely wrong training format

---

### Issue 6: json.dumps() Data Format Bug

**Discovery Date**: Early dataset generation
**Severity**: HIGH - Complete training failure

**Symptoms**:
- Model learns function schema instead of calls
- Generates: `type:<escape>STRING<escape>},song_name:{description:<escape>Name of the song<escape>}`
- Pattern recognition completely wrong
- Model outputs look like JSON schema

**Root Cause**:
Passing JSON string instead of Python dict to `apply_chat_template`. The template expects a dict and formats it correctly. Passing a string breaks the formatting.

**Wrong Code** (CAUSES FAILURE):
```python
import json

messages = [
    {"role": "user", "content": "Play Bohemian Rhapsody"},
    {
        "role": "assistant",
        "tool_calls": [{
            "type": "function",
            "function": {
                "name": "play_song",
                "arguments": json.dumps({"song_name": "Bohemian Rhapsody"})  # WRONG!
            }
        }]
    }
]

formatted = tokenizer.apply_chat_template(messages, tools=FUNCTIONS)
```

**What Happens**:
- `apply_chat_template` receives string `'{"song_name": "Bohemian Rhapsody"}'`
- Template doesn't parse the JSON string
- Outputs malformed training data
- Model learns wrong patterns

**Correct Code**:
```python
messages = [
    {"role": "user", "content": "Play Bohemian Rhapsody"},
    {
        "role": "assistant",
        "tool_calls": [{
            "type": "function",
            "function": {
                "name": "play_song",
                "arguments": {"song_name": "Bohemian Rhapsody"}  # Dict, not string!
            }
        }]
    }
]

formatted = tokenizer.apply_chat_template(messages, tools=FUNCTIONS)
```

**Produces Correct Format**:
```
<start_function_call>call:play_song{song_name:<escape>Bohemian Rhapsody<escape>}<end_function_call>
```

**Key Takeaway**: Always pass Python objects (dicts, lists) to `apply_chat_template`, never JSON strings.

---

### Issue 7: Cognitive Overload (18 Functions Simultaneously)

**Discovery Date**: Initial training attempt
**Severity**: HIGH - Fundamental limitation

**Experiment**:
- Trained with all 18 functions simultaneously
- 900 examples (50 per function)
- 5 epochs training
- Proper LoRA configuration
- Correct data format

**Result**:
- Training completed normally
- High token accuracy (98%+)
- **0% functional accuracy** on all tests
- Model couldn't distinguish between any functions

**Analysis**:
270M parameter model cannot learn 18 different function calling patterns simultaneously with limited examples per pattern.

**Evidence**:
| Functions | Examples | Accuracy | Status |
|-----------|----------|----------|--------|
| 18 at once | 900 | 0% | ‚ùå Failed |
| 2 gradual | 100 | 100% | ‚úÖ Success |
| 4 gradual | 100 | 98.9% | ‚úÖ Success |

**Solution**: Gradual scaling approach (2‚Üí4‚Üí8‚Üí18)
- Start with 2 functions
- Validate success
- Incrementally add functions
- Each stage builds on previous knowledge

**Why This Works**:
1. Model learns general function calling pattern early
2. Each stage adds manageable complexity
3. Validation gates prevent advancing on failure
4. Incremental learning prevents overload

---

## üîç Inference Debugging Details

### Three Working Approaches

After resolving test infrastructure issues, confirmed that **all three inference approaches work correctly**.

#### Approach 1: Standard PEFT Loading

**Method**: Load adapter on top of base model, use PEFT layers during inference.

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "google/functiongemma-270m-it",
    torch_dtype=torch.float32,  # float32 for CPU
    device_map="cpu",
    trust_remote_code=True
)

# Load adapter
tokenizer = AutoTokenizer.from_pretrained("Jageen/music-4func")
model = PeftModel.from_pretrained(base_model, "Jageen/music-4func")

# Generate
messages = [{"role": "user", "content": "Play Bohemian Rhapsody"}]
prompt = tokenizer.apply_chat_template(
    messages,
    tools=MUSIC_FUNCTIONS,
    add_generation_prompt=True,
    tokenize=False
)
inputs = tokenizer(prompt, return_tensors="pt")

outputs = model.generate(
    **inputs,
    max_new_tokens=128,
    do_sample=False,
    pad_token_id=tokenizer.eos_token_id
)

response = tokenizer.decode(outputs[0][inputs['input_ids'].shape[1]:])
```

**Result**: ‚úÖ SUCCESS
**Output**: `<start_function_call>call:play_song{song_name:<escape>Bohemian Rhapsody<escape>}<end_function_call>`

**Pros**:
- Smallest memory footprint
- Standard PEFT pattern
- Easy to swap adapters

**Cons**:
- Slightly slower (adapter computation overhead)
- Fails on GPU with float16 (see Issue 3)

---

#### Approach 2: Merged Adapters

**Method**: Merge LoRA weights into base model, creating standalone model.

```python
from peft import PeftModel
from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# Load base model
base_model = AutoModelForCausalLM.from_pretrained(
    "google/functiongemma-270m-it",
    torch_dtype=torch.float32,
    device_map="cpu",
    trust_remote_code=True
)

# Load and merge adapter
tokenizer = AutoTokenizer.from_pretrained("Jageen/music-4func")
peft_model = PeftModel.from_pretrained(base_model, "Jageen/music-4func")
model = peft_model.merge_and_unload()  # Merge LoRA weights

# Generate (same as Approach 1)
outputs = model.generate(**inputs, max_new_tokens=128, do_sample=False)
```

**Result**: ‚úÖ SUCCESS
**Output**: Same correct function call format

**Pros**:
- Faster inference (no adapter overhead)
- Works on GPU with float16
- Simpler model handling
- **RECOMMENDED FOR PRODUCTION**

**Cons**:
- Slightly more memory during merge
- Can't easily swap adapters

---

#### Approach 3: Different Generation Parameters

**Method**: Use sampling instead of greedy decoding.

```python
# Same model loading as Approach 1 or 2

# Generate with sampling
outputs = model.generate(
    **inputs,
    max_new_tokens=128,
    do_sample=True,      # Enable sampling
    temperature=0.7,
    top_p=0.9,
    pad_token_id=tokenizer.eos_token_id
)
```

**Result**: ‚úÖ SUCCESS
**Output**: Correct function calls (with slight variation due to sampling)

**Pros**:
- More diverse outputs
- Can help with edge cases

**Cons**:
- Non-deterministic (different each run)
- Usually unnecessary for function calling

---

### Debugging Script: `scripts/debug_inference.py`

Created systematic debugging script that tests all three approaches and provides detailed output.

**Usage**:
```bash
cd /path/to/music_app_training
source venv/bin/activate
python scripts/debug_inference.py
```

**Output Format**:
```
================================================================================
üîç DEBUGGING INFERENCE ISSUES
================================================================================

================================================================================
APPROACH 1: Standard PEFT Loading
================================================================================
‚úÖ Model loaded (3.8M trainable params)

Testing: "Play Bohemian Rhapsody"
‚úÖ Input shape: torch.Size([1, 69])
‚úÖ Device: cpu
‚úÖ Generated 129 characters
Response: <start_function_call>call:play_song{song_name:<escape>Bohemian Rhapsody<escape>}<end_function_call>
‚úÖ SUCCESS: Function call detected!

================================================================================
APPROACH 2: Merged Adapters
================================================================================
‚úÖ Adapters merged successfully

Testing: "Play Bohemian Rhapsody"
‚úÖ SUCCESS: Function call detected!

================================================================================
APPROACH 3: Different Generation Parameters
================================================================================
‚úÖ Using sampling (temperature=0.7)

Testing: "Play Bohemian Rhapsody"
‚úÖ SUCCESS: Function call detected!

================================================================================
üìä SUMMARY
================================================================================
Approach 1 (Standard PEFT): ‚úÖ WORKS
Approach 2 (Merged Adapters): ‚úÖ WORKS
Approach 3 (Different Params): ‚úÖ WORKS

‚úÖ ALL APPROACHES WORK! Model is functioning correctly.
```

**Use Cases**:
1. Verify model training success
2. Test after making configuration changes
3. Troubleshoot deployment issues
4. Compare inference approaches
5. Validate environment setup

---

## üîß Technical Configuration Reference

### Complete LoRA Configuration

```python
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training

lora_config = LoraConfig(
    r=16,                          # LoRA rank (higher = more capacity, slower)
    lora_alpha=32,                 # LoRA scaling factor (typically 2x rank)
    target_modules=[               # CRITICAL: All 7 modules required
        "q_proj",                  # Query projection (attention)
        "k_proj",                  # Key projection (attention)
        "v_proj",                  # Value projection (attention)
        "o_proj",                  # Output projection (attention)
        "gate_proj",               # Gate projection (FFN) - MUST INCLUDE
        "up_proj",                 # Up projection (FFN) - MUST INCLUDE
        "down_proj"                # Down projection (FFN) - MUST INCLUDE
    ],
    lora_dropout=0.05,             # Dropout for LoRA layers
    bias="none",                   # Don't train bias parameters
    task_type="CAUSAL_LM"         # Task type for model
)

# Apply to model
model = prepare_model_for_kbit_training(model)
model = get_peft_model(model, lora_config)

# Verify configuration
model.print_trainable_parameters()
# Expected output:
# trainable params: 3,796,992 || all params: 271,229,184 || trainable%: 1.4000
```

**Why These Specific Values**:
- `r=16`: Good balance of capacity vs speed for 270M model
- `lora_alpha=32`: Standard 2x rule for scaling
- `target_modules`: Must include all 7 for proper learning
- `lora_dropout=0.05`: Prevents overfitting on small datasets

---

### Complete Training Configuration

```python
from trl import SFTTrainer, SFTConfig
from pathlib import Path

# Output directory with timestamp
output_dir = Path(f"models/music-4func-{timestamp}")
output_dir.mkdir(parents=True, exist_ok=True)

# Training arguments
training_args = SFTConfig(
    output_dir=str(output_dir),
    num_train_epochs=5,                      # Number of training epochs
    per_device_train_batch_size=2,           # Batch size per device (train)
    per_device_eval_batch_size=2,            # Batch size per device (eval)
    gradient_accumulation_steps=4,           # Effective batch size = 2*4 = 8
    learning_rate=2e-4,                      # Learning rate (0.0002)
    weight_decay=0.01,                       # L2 regularization
    logging_dir=str(output_dir / "logs"),
    logging_steps=10,                        # Log every 10 steps
    eval_strategy="epoch",                   # Evaluate after each epoch
    save_strategy="epoch",                   # Save after each epoch
    save_total_limit=2,                      # Keep only 2 checkpoints
    load_best_model_at_end=True,             # Load best checkpoint at end
    report_to="none",                        # Don't report to external services
    fp16=False,                              # Don't use mixed precision (CPU)
)

# Formatting function
def formatting_func(example):
    """Extract pre-formatted text from example."""
    return example["text"]

# Create trainer
trainer = SFTTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=eval_dataset,
    processing_class=tokenizer,              # Tokenizer (new API)
    formatting_func=formatting_func,         # CRITICAL: Use pre-formatted text
)

# Train
trainer.train()

# Save final model
final_dir = output_dir / "final"
trainer.save_model(str(final_dir))
tokenizer.save_pretrained(str(final_dir))
```

**Training Time Expectations**:
- 2 functions (100 examples): ~2.5 minutes
- 4 functions (100 examples): ~2.5 minutes
- 8 functions (200 examples): ~4-5 minutes
- 18 functions (450 examples): ~10-12 minutes

---

### Complete Dataset Format

```python
from datasets import Dataset

# Example data
examples = [
    {
        "user_input": "Play Bohemian Rhapsody",
        "function_name": "play_song",
        "arguments": {"song_name": "Bohemian Rhapsody"}  # Dict, not JSON string!
    },
    # More examples...
]

# Format examples using tokenizer template
def format_example(example):
    messages = [
        {"role": "user", "content": example["user_input"]},
        {
            "role": "assistant",
            "tool_calls": [{
                "type": "function",
                "function": {
                    "name": example["function_name"],
                    "arguments": example["arguments"]  # Pass dict directly
                }
            }]
        }
    ]

    formatted_text = tokenizer.apply_chat_template(
        messages,
        tools=FUNCTIONS,                    # Function definitions
        tokenize=False,                     # Return string, not tokens
        add_generation_prompt=False         # Don't add prompt (training)
    )

    return {"text": formatted_text}

# Create dataset
data = [format_example(ex) for ex in examples]
dataset = Dataset.from_dict({"text": [d["text"] for d in data]})

# Split into train/eval
train_test = dataset.train_test_split(test_size=0.2, seed=42)
train_dataset = train_test["train"]
eval_dataset = train_test["test"]
```

**Formatted Output Example**:
```
<start_of_turn>user
Play Bohemian Rhapsody<end_of_turn>
<start_of_turn>model
<start_function_call>call:play_song{song_name:<escape>Bohemian Rhapsody<escape>}<end_function_call><end_of_turn>
```

---

## üìä All Function Definitions

Complete set of 18 music control functions used in training:

```python
MUSIC_FUNCTIONS = [
    {
        "type": "function",
        "function": {
            "name": "play_song",
            "description": "Play a specific song by name or artist",
            "parameters": {
                "type": "object",
                "properties": {
                    "song_name": {"type": "string", "description": "Name of the song"},
                    "artist": {"type": "string", "description": "Artist name (optional)"},
                    "album": {"type": "string", "description": "Album name (optional)"}
                },
                "required": ["song_name"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "playback_control",
            "description": "Control music playback",
            "parameters": {
                "type": "object",
                "properties": {
                    "action": {
                        "type": "string",
                        "enum": ["play", "pause", "skip", "next", "previous", "stop", "resume"]
                    }
                },
                "required": ["action"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "search_music",
            "description": "Search for music by query, artist, album, or genre",
            "parameters": {
                "type": "object",
                "properties": {
                    "query": {"type": "string", "description": "Search query"},
                    "type": {"type": "string", "enum": ["song", "artist", "album", "playlist", "genre"]}
                },
                "required": ["query"]
            }
        }
    },
    {
        "type": "function",
        "function": {
            "name": "create_playlist",
            "description": "Create a new playlist",
            "parameters": {
                "type": "object",
                "properties": {
                    "name": {"type": "string", "description": "Playlist name"}
                },
                "required": ["name"]
            }
        }
    },
    # ... 14 more functions (add_to_playlist, remove_from_playlist, etc.)
]
```

**Function Categories**:
1. **Playback** (2 functions): play_song, playback_control
2. **Search** (1 function): search_music
3. **Playlists** (4 functions): create_playlist, add_to_playlist, remove_from_playlist, get_queue
4. **Queue** (2 functions): clear_queue, add_to_queue
5. **Favorites** (3 functions): add_to_favorites, remove_from_favorites, get_favorites
6. **Controls** (4 functions): shuffle, repeat, set_volume, set_equalizer
7. **Info** (2 functions): get_current_song, get_recommendations

---

## üìà Training Results by Stage

### Stage 1: 2 Functions (COMPLETE)

**Configuration**:
- Functions: `play_song`, `playback_control`
- Examples: 100 (50 per function)
- Train/Eval split: 80/20
- Epochs: 5
- Batch size: 2
- Gradient accumulation: 4

**Results**:
```
Training Accuracy: 100%
Eval Accuracy: 100%
Training Time: 2 minutes 37 seconds
Final Loss: 0.0001
```

**Model**: [Jageen/music-2func](https://huggingface.co/Jageen/music-2func)

**Test Results**: 8/8 tests passed (100%)

---

### Stage 2: 4 Functions (COMPLETE)

**Configuration**:
- Functions: `play_song`, `playback_control`, `search_music`, `create_playlist`
- Examples: 100 (25 per function)
- Train/Eval split: 80/20
- Epochs: 5
- Batch size: 2
- Gradient accumulation: 4

**Results**:
```
Training Accuracy: 98.9%
Eval Accuracy: 98.5%
Training Time: 2 minutes 41 seconds
Final Loss: 0.0342
Trainable Parameters: 3,796,992 (1.40%)
Adapter Size: 14.5 MB
```

**Model**: [Jageen/music-4func](https://huggingface.co/Jageen/music-4func)

**Test Results**: 8/8 tests passed (100% on validation set)

**Local Demo Results**:
```
Base FunctionGemma: 75% (6/8 tests)
Fine-Tuned Model: 100% (8/8 tests)
Improvement: +25 percentage points
```

---

### Stage 3: 8 Functions (PLANNED)

**Configuration**:
- Functions: All from Stage 2 + `add_to_playlist`, `remove_from_playlist`, `shuffle`, `repeat`
- Examples: 200 (25 per function)
- Expected accuracy: 95%+
- Expected time: ~4-5 minutes

---

### Stage 4: 18 Functions (PLANNED)

**Configuration**:
- Functions: All 18 music control functions
- Examples: 450+ (25 per function)
- Expected accuracy: 90%+
- Expected time: ~10-12 minutes

---

## ‚è±Ô∏è Development Timeline

### Week 1: Initial Attempts
- Attempted training all 18 functions at once
- Result: Complete failure (0% accuracy)
- Diagnosed cognitive overload issue
- Designed gradual scaling approach

### Week 2: 2-Function Success
- Implemented 2-function training
- Discovered LoRA configuration issue
- Fixed target_modules to include all 7
- Achieved 100% accuracy
- Uploaded to HuggingFace: Jageen/music-2func

### Week 3: 4-Function Success
- Extended to 4 functions
- Discovered test infrastructure issues
- Created comprehensive debugging script
- Confirmed inference works correctly
- Achieved 98.9% accuracy
- Uploaded to HuggingFace: Jageen/music-4func

### Week 4: Documentation & Preparation
- Created local demo script
- Fixed Colab notebook for GPU
- Discovered merge_and_unload() requirement for GPU
- Documented all issues and solutions
- Prepared for public release

---

## üí° Key Learnings

### Technical Learnings

1. **LoRA Configuration is Critical**
   - Must include all 7 target modules
   - Missing modules = complete failure
   - Verify adapter size (~15MB for correct config)

2. **Gradual Scaling Works**
   - Small models can't learn too many patterns at once
   - Start with 2-4 functions, validate, then scale
   - Each stage builds on previous knowledge

3. **Data Format Matters**
   - Pass dicts to `apply_chat_template`, never JSON strings
   - Use `formatting_func` in SFTTrainer
   - Verify formatted output before training

4. **Token Accuracy ‚â† Functional Accuracy**
   - High token accuracy doesn't guarantee correct output
   - Always test actual function calling
   - Use systematic validation

5. **Test Infrastructure First**
   - Verify tests work before diagnosing model issues
   - Use multiple test approaches
   - Don't assume environment/library bugs

6. **GPU Inference Requires Merging**
   - PEFT + GPU + float16 = pad tokens only
   - Use `merge_and_unload()` for GPU inference
   - Merged model is faster anyway

### Process Learnings

1. **Document Everything**
   - Record all errors and solutions
   - Future you will thank present you
   - Helps others avoid same mistakes

2. **Systematic Debugging**
   - Test multiple approaches
   - Isolate variables
   - Don't make assumptions

3. **Validation Gates**
   - Don't scale until current stage succeeds
   - Each stage must pass tests
   - Prevents wasted effort

4. **Start Simple**
   - Smallest possible working example first
   - Add complexity incrementally
   - Validate at each step

### Deployment Learnings

1. **HuggingFace is Essential**
   - Easy sharing and deployment
   - Bypass local environment issues
   - Version control for models

2. **Colab for GPU Testing**
   - Free GPU access
   - Reproducible environment
   - Great for demos

3. **Local Demo Scripts**
   - Perfect for blog screenshots
   - Shows before/after clearly
   - Easy to run and verify

---

## üîó Additional Resources

### Training Logs

All training logs are stored in `.archive/logs/`:
- `training_4func.log` - Final successful 4-function training
- `training_2func.log` - 2-function training
- `debug_inference_results.log` - Inference debugging results

### Archived Scripts

Development/test scripts in `.archive/scripts/`:
- Early dataset generation attempts
- Old training configurations
- Various test scripts
- Ollama conversion experiments

### Archived Documentation

Development docs in `.archive/docs/`:
- `INFERENCE_DEBUGGING_SUMMARY.md` - Detailed debugging notes
- `LOCAL_WORKFLOW.md` - Development workflow
- `TESTING_GUIDE.md` - Testing procedures
- Various experimental notebooks

---

## üéØ Next Development Steps

1. **Complete 8-Function Training**
   - Create `data/eight_func_examples.py`
   - Generate dataset with `scripts/generate_8func_dataset.py`
   - Train and validate
   - Upload to HuggingFace

2. **Complete 18-Function Training**
   - Create comprehensive examples (450+)
   - Train final model
   - Extensive validation
   - Document any new findings

3. **Production Optimization**
   - Test on mobile devices
   - Optimize inference speed
   - Reduce model size if needed
   - Create deployment guides

4. **Expand to Other Domains**
   - Apply gradual scaling to other function sets
   - Test with different base models
   - Document domain-specific findings

---

**Document Last Updated**: 2026-01-05

**Status**: 4-function model complete and deployed. Ready for 8-function stage.

**Contact**: See main README.md for contribution guidelines.
